{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.txt', sep=';')\n",
    "test_df = pd.read_csv('data/test.txt', sep=';')\n",
    "val_df = pd.read_csv('data/val.txt', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80788ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, test_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd92253",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = ['sentence', 'emotion']\n",
    "test_df.columns = ['sentence', 'emotion']\n",
    "val_df.columns = ['sentence', 'emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max len of sentences\n",
    "def max_len(data):\n",
    "    return data['sentence'].apply(lambda x: len(x.split())).max()\n",
    "\n",
    "max_lens = [max_len(train_df), max_len(test_df), max_len(val_df)]\n",
    "max_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55718dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUT_DIR = 'model_output'\n",
    "TRAIN_FILE_PATH = 'data/train.txt'\n",
    "VALID_FILE_PATH = 'data/val.txt'\n",
    "TEST_FILE_PATH = 'data/test.txt'\n",
    "\n",
    "## Model Configurations\n",
    "MAX_LEN_TRAIN = 68\n",
    "MAX_LEN_VALID = 68\n",
    "MAX_LEN_TEST = 68\n",
    "BATCH_SIZE = 160\n",
    "LR = 1e-5\n",
    "NUM_EPOCHS = 10\n",
    "NUM_THREADS = 1  ## Number of threads for collecting dataset\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "LABEL_DICT = {'joy':0, 'sadness':1, 'anger':2, 'fear':3, 'love':4, 'surprise':5}\n",
    "\n",
    "if not os.path.isdir(MODEL_OUT_DIR):\n",
    "    os.makedirs(MODEL_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7262664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emotions_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, maxlen, tokenizer, label_dict): \n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = pd.read_csv(filename, delimiter = ';')\n",
    "        # name columns\n",
    "        self.df.columns = ['sentence', 'emotion']\n",
    "        #Initialize the tokenizer for the desired transformer model\n",
    "        self.df['emotion'] = self.df['emotion'].map(label_dict)\n",
    "        self.tokenizer = tokenizer\n",
    "        #Maximum length of the tokens list to keep all the sequences of fixed size\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        #Select the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'sentence']\n",
    "        label = self.df.loc[index, 'emotion']\n",
    "        #Preprocess the text to be suitable for the transformer\n",
    "        tokens = self.tokenizer.tokenize(sentence) \n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]'] \n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \n",
    "        #Obtain the indices of the tokens in the BERT Vocabulary\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \n",
    "        input_ids = torch.tensor(input_ids) \n",
    "        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmotionClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        #The classification layer that takes the [CLS] representation and outputs the logit\n",
    "        self.cls_layer = nn.Linear(config.hidden_size, 6)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #Feed the input to Bert model to obtain contextualized representations\n",
    "        reps, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #Obtain the representations of [CLS] heads\n",
    "        cls_reps = reps[:, 0]\n",
    "        logits = self.cls_layer(cls_reps)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, val_loader, epochs, device):\n",
    "    best_acc = 0\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        train_acc = 0\n",
    "        for i, (input_ids, attention_mask, labels) in enumerate(iterable=train_loader):\n",
    "            optimizer.zero_grad()  \n",
    "            \n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_acc += get_accuracy_from_logits(logits, labels)\n",
    "        \n",
    "        print(f\"Training accuracy is {train_acc/len(train_loader)}\")\n",
    "        val_acc, val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n",
    "        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(epoch, val_acc, val_loss))\n",
    "        \n",
    "#         if val_acc > best_acc:\n",
    "#             print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, val_acc))\n",
    "#             best_acc = val_acc\n",
    "#             model.save_pretrained(save_directory=MODEL_OUT_DIR + '/')\n",
    "#             config.save_pretrained(save_directory=MODEL_OUT_DIR + '/')\n",
    "#             tokenizer.save_pretrained(save_directory=MODEL_OUT_DIR + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    mean_acc, mean_loss, count = 0, 0, 0\n",
    "#     predicted_labels = []\n",
    "#     actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in (dataloader):\n",
    "            \n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            mean_loss += criterion(logits.squeeze(-1), labels).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "            \n",
    "#             predicted_labels += output\n",
    "#             actual_labels += labels\n",
    "            \n",
    "    return mean_acc/count, mean_loss/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    output = torch.argmax(probs, dim=1)\n",
    "    acc = (output == labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device):\n",
    "    predicted_label = []\n",
    "    actual_label = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in (dataloader):\n",
    "            \n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            output = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            predicted_label += output\n",
    "            actual_label += labels\n",
    "            \n",
    "    return predicted_label, actual_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration loaded from AutoConfig \n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "## Tokenizer loaded from AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "## Creating the model from the desired transformer model\n",
    "model = BertEmotionClassifier.from_pretrained(MODEL_NAME, config=config)\n",
    "## GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## Putting model to device\n",
    "model = model.to(device)\n",
    "## Takes as the input the logits of the positive class and computes the binary cross-entropy \n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "## Optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Dataset\n",
    "train_set = Emotions_Dataset(filename=TRAIN_FILE_PATH, maxlen=MAX_LEN_TRAIN, tokenizer=tokenizer, label_dict=LABEL_DICT)\n",
    "valid_set = Emotions_Dataset(filename=VALID_FILE_PATH, maxlen=MAX_LEN_VALID, tokenizer=tokenizer, label_dict=LABEL_DICT)\n",
    "test_set = Emotions_Dataset(filename=TEST_FILE_PATH, maxlen=MAX_LEN_TEST, tokenizer=tokenizer, label_dict=LABEL_DICT)\n",
    "\n",
    "\n",
    "## Data Loaders\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\n",
    "valid_loader = DataLoader(dataset=valid_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\n",
    "\n",
    "# print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model, \n",
    "      criterion=criterion,\n",
    "      optimizer=optimizer, \n",
    "      train_loader=train_loader,\n",
    "      val_loader=valid_loader,\n",
    "      epochs = 5,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-africa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
